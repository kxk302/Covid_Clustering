{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid_Clustering",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOB6KRyuxwQAjiLYaC+gu/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/Covid_Clustering/blob/main/Covid_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMNSRwAXRA7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5caa78-317c-4820-de93-02d2a7f761e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUlisGxsRloh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c3262d-57a2-42f1-8468-07587a66079b"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/Clustering'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boston_data  boston_results  uk_data  uk_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay6Ll_k6VBlf"
      },
      "source": [
        "import ast\n",
        "import bokeh.models as bmo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from bokeh.palettes import d3\n",
        "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# When calculating the distance between 2 probability densities,\n",
        "# if one probability value is 0 (or very small), the cross entropy\n",
        "# (distance) value would be infinity. This brakes the DBSCAN algorithm.\n",
        "# Replace infinity values with a large number, say 200.00\n",
        "max_distance = 200.00\n",
        "\n",
        "# x axis values for calculating/plotting KDE of sample AF\n",
        "x_idx = np.linspace(0.00, 1.00, num=100).tolist()\n",
        "\n",
        "def get_kde_values(row):\n",
        "  return gaussian_kde(row['AF']).evaluate(x_idx).tolist()\n",
        "\n",
        "def get_kl_div(x, y):\n",
        "  return entropy(x, y)\n",
        "\n",
        "def preprocess(file_name, sep=\"\\t\"):\n",
        "\n",
        "  # Read the input file. Select only the needed columns.\n",
        "  df = pd.read_csv(file_name, sep)[['Sample' , 'AF']]\n",
        "  df_in = df.copy()\n",
        "\n",
        "  # Sample stats\n",
        "  print('\\n')\n",
        "  print('Number of unique samples: {}'.format(df_in['Sample'].nunique()))\n",
        "  print('Sample minimum: {}'.format(df_in['Sample'].min()))\n",
        "  print('Sample maximum: {}'.format(df_in['Sample'].max()))\n",
        "\n",
        "  # af stats\n",
        "  print('\\n')\n",
        "  print('Number of unique af {}'.format(df_in['AF'].nunique()))\n",
        "  print('af minimum: {}'.format(df_in['AF'].min()))\n",
        "  print('af maximum: {}'.format(df_in['AF'].max()))\n",
        "\n",
        "  # Clean up data by removing rows where af is greater than 1.0\n",
        "  print('\\n')\n",
        "  print('Removing rows with AF greater than 1.0')\n",
        "  df_af = df_in[df_in.AF <= 1.00]\n",
        "\n",
        "  # Pivot the data frame\n",
        "  df_piv = pd.pivot_table(df_af, index='Sample', values='AF', aggfunc=list)\n",
        "  print('df_piv.head(5)')\n",
        "  print(df_piv.head(5))\n",
        "\n",
        "  # Clean up data by removing rows where af list has only one or two element\n",
        "  # KDE calculation errors out for those\n",
        "  df_piv_clean = df_piv[ df_piv.AF.str.len() > 2]\n",
        "\n",
        "  # Calculate \n",
        "  df_piv_clean['KDE_vals'] = df_piv_clean.apply(get_kde_values, axis=1)\n",
        "\n",
        "  print('df_piv_clean.head(5)')\n",
        "  print(df_piv_clean.head(5))\n",
        "\n",
        "  return df_piv_clean\n",
        "\n",
        "# \"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", sep=\"\\t\"\n",
        "# eps: 0.0025 for 5 clusters in Boston dataset\n",
        "# metric: get_kl_div\n",
        "\n",
        "# eps: \n",
        "#   The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
        "# min_samples: \n",
        "#   The number of samples n a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
        "# metric: \n",
        "#   The metric to use when calculating distance between instances in a feature array. \n",
        "# metric_params: \n",
        "#  Additional keyword arguments for the metric function.\n",
        "\n",
        "def dbscan_clustering(file_name, sep='\\t', eps=0.5, min_samples=5, metric='euclidean', metric_params=None):\n",
        "  df_piv_clean = preprocess(file_name, sep)\n",
        "\n",
        "  if metric == 'precomputed':\n",
        "    distances, _ = get_distance_matrix(df_piv_clean)\n",
        "\n",
        "    # Replace infinity values in distances matrix with a large value\n",
        "    distances.replace([np.inf], max_distance, inplace=True)\n",
        "\n",
        "    # Run DBSCAN clustering algorithm on precomputed distance matric\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params).fit(distances) \n",
        "  else:\n",
        "    # Run DBSCAN clustering algorithm\n",
        "    db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params).fit(df_piv_clean.KDE_vals.tolist())\n",
        "\n",
        "  labels = db.labels_\n",
        "\n",
        "  # Number of clusters in labels, ignoring noise if present.\n",
        "  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "  n_noise_ = list(labels).count(-1)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Number of clusters: {}'.format(n_clusters_))\n",
        "  print('Cluster labels: {}'.format(set(labels)))\n",
        "  print('Number of noise samples: {}'.format(n_noise_))\n",
        "\n",
        "  # Add Labels (and its string version) to the dataframe\n",
        "  df_piv_clean['Labels'] = labels\n",
        "\n",
        "  print('df_piv_clean.head(5)')\n",
        "  print(df_piv_clean.head(5))\n",
        "\n",
        "  return df_piv_clean\n",
        "\n",
        "def get_distance_matrix(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  row_count = df.shape[0]\n",
        "  distances = np.zeros((row_count, row_count))\n",
        "\n",
        "  for idx1 in range(row_count-1):\n",
        "    for idx2 in range(idx1+1, row_count):\n",
        "      distances[idx1][idx2] = entropy(df.iloc[idx1]['KDE_vals'], df.iloc[idx2]['KDE_vals'])\n",
        "      distances[idx2][idx1] = distances[idx1][idx2]\n",
        "  \n",
        "  df_out = pd.DataFrame(distances)\n",
        "  distances_sum = df_out.apply(np.sum)\n",
        "  argmin = distances_sum.argmin()\n",
        "  return df_out, df.iloc[argmin]\n",
        "\n",
        "def plot_clusters(df_in, folder):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  num_labels = df['Labels'].nunique()\n",
        "  print('num_labels: {}'.format(num_labels))\n",
        "\n",
        "  labels = df['Labels'].unique()\n",
        "  print('labels: {}'.format(labels))\n",
        "\n",
        "  fig, axs = plt.subplots(num_labels, 2, gridspec_kw={'hspace': 1.0, 'wspace': 0.5}, figsize=(15, 15))\n",
        "\n",
        "  # Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "  for label in labels:\n",
        "    print('Label processed: {}'.format(label))\n",
        "\n",
        "    # idx used in plot axes\n",
        "    idx = 0\n",
        "    if label != -1:\n",
        "      idx = label\n",
        "    else:\n",
        "      idx = num_labels - 1\n",
        "\n",
        "    df_lbl = df[ df.Labels == label ]\n",
        "    \n",
        "    distances, cluster_center = get_distance_matrix(df_lbl)\n",
        "    print('Cluster center for label ' + str(label))\n",
        "    print(cluster_center)\n",
        "    \n",
        "    # Histogram\n",
        "    xh = cluster_center[0]\n",
        "    axs[idx][0].hist(xh, density=True)\n",
        "    axs[idx][0].title.set_text('Cluster ' + str(label) + ' (size ' + str(df_lbl.shape[0]) + ') AF histogram')\n",
        "\n",
        "    # KDE \n",
        "    xk = x_idx\n",
        "    yk = cluster_center[1]\n",
        "    axs[idx][1].plot(xk, yk)\n",
        "    axs[idx][1].title.set_text('Cluster ' + str(label) + ' (size ' + str(df_lbl.shape[0]) + ') AF density estimate')\n",
        "    \n",
        "  plt.savefig(folder + '/dbscan_' + str(num_labels) + '.png')\n",
        "  plt.show()\n",
        "  \n",
        "def get_cluster_positions(df_in, cluster_label, file_name, sep):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "  df = df[ df.Labels == cluster_label ]\n",
        "\n",
        "  # Read the input file. Select only the needed columns.\n",
        "  df_positions = pd.read_csv(file_name, sep)[['Sample' , 'POS']]\n",
        "  return df_positions[ df_positions.Sample.isin(df.index.tolist()) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91auKrzlxfl"
      },
      "source": [
        "# Run DBSCAN clustering algorithm on Boston Covid-19 dataset\n",
        "#\n",
        "# Using 0.0025 for esp (epsilon) to yield 5 clusters\n",
        "# Using default value of 5 for min_samples \n",
        "# Using get_kl_div method for metric. get_kl_div() calculates Kullback-Leibler divergence, which measures the distance between 2 proabaility distributions\n",
        "# Using None for metric_params, as the metric has no parameters\n",
        "#\n",
        "\n",
        "# 1. Cleaned the data (Removed rows with AF > 1.0)\n",
        "# 2. Pivoted the data so all AFs of a sample are listed on one line\n",
        "# 3. Calculated Kernel Density Estimates (KDE) of AFs of each sample\n",
        "#      Evaluated them on 100 data points in range of 0.0 to 1.0\n",
        "# 4. Ran DBSCAN clustering algorithm\n",
        "#      epsilon: 0.0025\n",
        "#      Used Kullback-Liebler (KL) div. to calculate distance between density estimate\n",
        "# 5. DBSCAN produced 5 clusters\n",
        "#      Data points not assigned to any cluster marked as Noise (or cluster -1)\n",
        "# 6. For each cluster, found a representative sample\n",
        "#      Calculated KL div. between every pair of points in a cluster\n",
        "#      Selected point  with smallest sum of distance\n",
        "\n",
        "df = dbscan_clustering(file_name='https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz', \n",
        "                       sep='\\t', \n",
        "                       eps=0.0025, \n",
        "                       min_samples=5, \n",
        "                       #metric=get_kl_div,\n",
        "                       metric='precomputed',\n",
        "                       metric_params=None)\n",
        "\n",
        "plot_clusters(df, folder='/content/gdrive/MyDrive/Colab Notebooks/Clustering/boston_results')\n",
        "\n",
        "labels = df['Labels'].unique()\n",
        "print('labels: {}'.format(labels))\n",
        "\n",
        "# Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "for label in labels:\n",
        "  print('Label processed: {}'.format(label))\n",
        "\n",
        "  # Get positions for cluster 3 samples \n",
        "  cluster_df = get_cluster_positions(df_in=df, cluster_label=label, file_name='https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz', sep='\\t')\n",
        "  cluster_df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/Clustering/boston_results/cluster_' + str(label) + '.csv', sep=',', index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run DBSCAN clustering algorithm on UK Covid-19 dataset\n",
        "#\n",
        "# Using 0.0025 for esp (epsilon) to yield 5 clusters\n",
        "# Using default value of 5 for min_samples \n",
        "# Using get_kl_div method for metric. get_kl_div() calculates Kullback-Leibler divergence, which measures the distance between 2 proabaility distributions\n",
        "# Using None for metric_params, as the metric has no parameters\n",
        "#\n",
        "df = dbscan_clustering(file_name='/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_data/data.tsv', \n",
        "                       sep='\\t', \n",
        "                       eps=0.0070, \n",
        "                       min_samples=5, \n",
        "                       #metric=get_kl_div,\n",
        "                       metric='precomputed',\n",
        "                       metric_params=None)\n",
        "\n",
        "df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_results/all_clusters_eps_0070_min_samples_5.tsv', sep='\\t')\n",
        "\n",
        "plot_clusters(df, folder='/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_results')\n",
        "\n",
        "labels = df['Labels'].unique()\n",
        "print('labels: {}'.format(labels))\n",
        "\n",
        "# Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "for label in labels:\n",
        "  print('Label processed: {}'.format(label))\n",
        "\n",
        "  # Get positions for cluster 3 samples \n",
        "  cluster_df = get_cluster_positions(df_in=df, cluster_label=label, file_name='/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_data/data.tsv', sep='\\t')\n",
        "  cluster_df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_results/cluster_' + str(label) + '.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "QToDZbybVq06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}