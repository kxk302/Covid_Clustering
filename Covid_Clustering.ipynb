{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid_Clustering",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFbmr8qA0DP0ONEmr+GkG7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/Covid_Clustering/blob/main/Covid_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMNSRwAXRA7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74be599-b09f-4024-a5f1-4a0e21290e10"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUlisGxsRloh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3961eb-bb40-4a6e-8343-042544bedda4"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/Clustering'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boston_data  boston_results  uk_data  uk_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay6Ll_k6VBlf"
      },
      "source": [
        "import ast\n",
        "import bokeh.models as bmo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from bokeh.palettes import d3\n",
        "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# x axis values for calculating/plotting KDE of sample AF\n",
        "x_idx = np.linspace(0.00, 1.00, num=100).tolist()\n",
        "\n",
        "def get_kde_values(row):\n",
        "  return gaussian_kde(row['AF']).evaluate(x_idx).tolist()\n",
        "\n",
        "def get_kl_div(x, y):\n",
        "  return entropy(x, y)\n",
        "\n",
        "# \"https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz\", sep=\"\\t\"\n",
        "# eps: 0.0025 for 5 clusters in Boston dataset\n",
        "# metric: get_kl_div\n",
        "\n",
        "# eps: \n",
        "#   The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
        "# min_samples: \n",
        "#   The number of samples n a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
        "# metric: \n",
        "#   The metric to use when calculating distance between instances in a feature array. \n",
        "# metric_params: \n",
        "#  Additional keyword arguments for the metric function.\n",
        "\n",
        "def dbscan_clustering(file_name, sep='\\t', eps=0.5, min_samples=5, metric='euclidean', metric_params=None):\n",
        "  # Read the input file. Select only the needed columns.\n",
        "  df_in = pd.read_csv(file_name, sep)[['Sample' , 'AF']]\n",
        "\n",
        "  # Sample stats\n",
        "  print('\\n')\n",
        "  print('Number of unique samples: {}'.format(df_in['Sample'].nunique()))\n",
        "  print('Sample minimum: {}'.format(df_in['Sample'].min()))\n",
        "  print('Sample maximum: {}'.format(df_in['Sample'].max()))\n",
        "\n",
        "  # af stats\n",
        "  print('\\n')\n",
        "  print('Number of unique af {}'.format(df_in['AF'].nunique()))\n",
        "  print('af minimum: {}'.format(df_in['AF'].min()))\n",
        "  print('af maximum: {}'.format(df_in['AF'].max()))\n",
        "\n",
        "  # Clean up data by removing rows where af is greater than 1.0\n",
        "  print('\\n')\n",
        "  print('Removing rows with AF greater than 1.0')\n",
        "  df = df_in[df_in.AF <= 1.00]\n",
        "\n",
        "  # Pivot the data frame\n",
        "  df_piv = pd.pivot_table(df, index='Sample', values='AF', aggfunc=list)\n",
        "  print('df_piv.head(5)')\n",
        "  print(df_piv.head(5))\n",
        "\n",
        "  # Clean up data by removing rows where af list has only one or two element\n",
        "  # KDE calculation errors out for those\n",
        "  df_piv_clean = df_piv[ df_piv.AF.str.len() > 2]\n",
        "\n",
        "  # Calculate \n",
        "  df_piv_clean['KDE_vals'] = df_piv_clean.apply(get_kde_values, axis=1)\n",
        "\n",
        "  print('df_piv_clean.head(5)')\n",
        "  print(df_piv_clean.head(5))\n",
        "\n",
        "  # Run DBSCAN clustering algorithm\n",
        "  db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params).fit(df_piv_clean.KDE_vals.tolist())\n",
        "  labels = db.labels_\n",
        "\n",
        "  # Number of clusters in labels, ignoring noise if present.\n",
        "  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "  n_noise_ = list(labels).count(-1)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Number of clusters: {}'.format(n_clusters_))\n",
        "  print('Cluster labels: {}'.format(set(labels)))\n",
        "  print('Number of noise samples: {}'.format(n_noise_))\n",
        "\n",
        "  # Add Labels (and its string version) to the dataframe\n",
        "  df_piv_clean['Labels'] = labels\n",
        "\n",
        "  print('df_piv_clean.head(5)')\n",
        "  print(df_piv_clean.head(5))\n",
        "\n",
        "  return df_piv_clean\n",
        "\n",
        "def get_distance_matrix(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  row_count = df.shape[0]\n",
        "  distances = np.zeros((row_count, row_count))\n",
        "\n",
        "  for idx1 in range(row_count-1):\n",
        "    for idx2 in range(idx1+1, row_count):\n",
        "      distances[idx1][idx2] = entropy(df.iloc[idx1]['KDE_vals'], df.iloc[idx2]['KDE_vals'])\n",
        "      distances[idx2][idx1] = distances[idx1][idx2]\n",
        "  \n",
        "  df_out = pd.DataFrame(distances)\n",
        "  distances_sum = df_out.apply(np.sum)\n",
        "  argmin = distances_sum.argmin()\n",
        "  return df_out, df.iloc[argmin]\n",
        "\n",
        "def plot_clusters(df_in, folder):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  num_labels = df['Labels'].nunique()\n",
        "  print('num_labels: {}'.format(num_labels))\n",
        "\n",
        "  labels = df['Labels'].unique()\n",
        "  print('labels: {}'.format(labels))\n",
        "\n",
        "  fig, axs = plt.subplots(num_labels, 2, gridspec_kw={'hspace': 1.0, 'wspace': 0.5}, figsize=(15, 15))\n",
        "\n",
        "  # Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "  for label in labels:\n",
        "    print('Label processed: {}'.format(label))\n",
        "\n",
        "    # idx used in plot axes\n",
        "    idx = 0\n",
        "    if label != -1:\n",
        "      idx = label\n",
        "    else:\n",
        "      idx = num_labels - 1\n",
        "\n",
        "    df_lbl = df[ df.Labels == label ]\n",
        "    \n",
        "    distances, cluster_center = get_distance_matrix(df_lbl)\n",
        "    print('Cluster center for label ' + str(label))\n",
        "    print(cluster_center)\n",
        "    \n",
        "    # Histogram\n",
        "    xh = cluster_center[0]\n",
        "    axs[idx][0].hist(xh, density=True)\n",
        "    axs[idx][0].title.set_text('Cluster ' + str(label) + ' (size ' + str(df_lbl.shape[0]) + ') AF histogram')\n",
        "\n",
        "    # KDE \n",
        "    xk = x_idx\n",
        "    yk = cluster_center[1]\n",
        "    axs[idx][1].plot(xk, yk)\n",
        "    axs[idx][1].title.set_text('Cluster ' + str(label) + ' (size ' + str(df_lbl.shape[0]) + ') AF density estimate')\n",
        "    \n",
        "  plt.savefig(folder + '/dbscan_' + str(num_labels) + '.png')\n",
        "  plt.show()\n",
        "  \n",
        "def get_cluster_positions(df_in, cluster_label, file_name, sep):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "  \n",
        "  df = df_in.copy()\n",
        "  df = df[ df.Labels == cluster_label ]\n",
        "\n",
        "  # Read the input file. Select only the needed columns.\n",
        "  df_positions = pd.read_csv(file_name, sep)[['Sample' , 'POS']]\n",
        "  return df_positions[ df_positions.Sample.isin(df.index.tolist()) ]\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91auKrzlxfl"
      },
      "source": [
        "# Run DBSCAN clustering algorithm on Boston Covid-19 dataset\n",
        "#\n",
        "# Using 0.0025 for esp (epsilon) to yield 5 clusters\n",
        "# Using default value of 5 for min_samples \n",
        "# Using get_kl_div method for metric. get_kl_div() calculates Kullback-Leibler divergence, which measures the distance between 2 proabaility distributions\n",
        "# Using None for metric_params, as the metric has no parameters\n",
        "#\n",
        "df = dbscan_clustering(file_name='https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz', \n",
        "                       sep='\\t', \n",
        "                       eps=0.0025, \n",
        "                       min_samples=5, \n",
        "                       metric=get_kl_div,\n",
        "                       metric_params=None)\n",
        "\n",
        "plot_clusters(d, folder='/content/gdrive/MyDrive/Colab Notebooks/Clustering/boston_results')\n",
        "\n",
        "labels = df['Labels'].unique()\n",
        "print('labels: {}'.format(labels))\n",
        "\n",
        "# Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "for label in labels:\n",
        "  print('Label processed: {}'.format(label))\n",
        "\n",
        "  # Get positions for cluster 3 samples \n",
        "  cluster_df = get_cluster_positions(df_in=df, cluster_label=label, file_name='https://github.com/galaxyproject/SARS-CoV-2/raw/master/data/var/bos_by_sample.tsv.gz', sep='\\t')\n",
        "  cluster_df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/Clustering/boston_results/cluster_' + str(label) + '.csv', sep=',', index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run DBSCAN clustering algorithm on UK Covid-19 dataset\n",
        "#\n",
        "# Using 0.0025 for esp (epsilon) to yield 5 clusters\n",
        "# Using default value of 5 for min_samples \n",
        "# Using get_kl_div method for metric. get_kl_div() calculates Kullback-Leibler divergence, which measures the distance between 2 proabaility distributions\n",
        "# Using None for metric_params, as the metric has no parameters\n",
        "#\n",
        "df = dbscan_clustering(file_name='/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_data/data_10_per.tsv', \n",
        "                       sep='\\t', \n",
        "                       eps=0.0025, \n",
        "                       min_samples=5, \n",
        "                       metric=get_kl_div,\n",
        "                       metric_params=None)\n",
        "\n",
        "plot_clusters(df, folder='/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_results')\n",
        "\n",
        "labels = df['Labels'].unique()\n",
        "print('labels: {}'.format(labels))\n",
        "\n",
        "# Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "for label in labels:\n",
        "  print('Label processed: {}'.format(label))\n",
        "\n",
        "  # Get positions for cluster 3 samples \n",
        "  cluster_df = get_cluster_positions(df_in=df, cluster_label=label, file_name='/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_data/data_10_per.tsv', sep='\\t')\n",
        "  cluster_df.to_csv('/content/gdrive/MyDrive/Colab Notebooks/Clustering/uk_results/cluster_' + str(label) + '.tsv', sep='\\t', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QToDZbybVq06",
        "outputId": "d3b44f07-2917-4a95-9038-142bd211adf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Number of unique samples: 19855\n",
            "Sample minimum: ERR5388794\n",
            "Sample maximum: ERR5694900\n",
            "\n",
            "\n",
            "Number of unique af 8400\n",
            "af minimum: 0.05\n",
            "af maximum: 594.6389\n",
            "\n",
            "\n",
            "Removing rows with AF greater than 1.0\n",
            "df_piv.head(5)\n",
            "                                                           AF\n",
            "Sample                                                       \n",
            "ERR5388794  [0.8444, 0.8944, 0.8112, 0.9633, 0.972, 0.9751...\n",
            "ERR5388795  [0.8929, 0.0552, 0.8959999999999999, 0.7854, 0...\n",
            "ERR5388796  [0.8596, 0.8929, 0.7951, 0.9667, 0.9501, 0.965...\n",
            "ERR5388797  [0.8533, 0.9074, 0.723, 0.8108, 0.9675, 0.9729...\n",
            "ERR5388798  [0.8789, 0.8966, 0.795, 0.9690000000000001, 0....\n",
            "df_piv_clean.head(5)\n",
            "                                                           AF  \\\n",
            "Sample                                                          \n",
            "ERR5388794  [0.8444, 0.8944, 0.8112, 0.9633, 0.972, 0.9751...   \n",
            "ERR5388795  [0.8929, 0.0552, 0.8959999999999999, 0.7854, 0...   \n",
            "ERR5388796  [0.8596, 0.8929, 0.7951, 0.9667, 0.9501, 0.965...   \n",
            "ERR5388797  [0.8533, 0.9074, 0.723, 0.8108, 0.9675, 0.9729...   \n",
            "ERR5388798  [0.8789, 0.8966, 0.795, 0.9690000000000001, 0....   \n",
            "\n",
            "                                                     KDE_vals  \n",
            "Sample                                                         \n",
            "ERR5388794  [0.10554159444378977, 0.11671239956014046, 0.1...  \n",
            "ERR5388795  [0.09828870300425267, 0.1063938589972826, 0.11...  \n",
            "ERR5388796  [9.05910775930019e-49, 3.3505753299060943e-47,...  \n",
            "ERR5388797  [3.6472329836018726e-107, 3.171440439044131e-1...  \n",
            "ERR5388798  [2.7338335595726234e-89, 7.692147363294878e-87...  \n"
          ]
        }
      ]
    }
  ]
}