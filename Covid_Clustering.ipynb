{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid_Clustering",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOfsLv+5PKoT649pk+ybyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kxk302/Covid_Clustering/blob/batch/Covid_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMNSRwAXRA7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164037d4-ca9a-4d96-9229-dedc21893386"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUlisGxsRloh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfef0e5a-84f8-48a7-f923-5f24d37de943"
      },
      "source": [
        "!ls '/content/gdrive/MyDrive/Colab Notebooks/Clustering/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_data  batch_results  boston_data\tboston_results\tuk_data  uk_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay6Ll_k6VBlf"
      },
      "source": [
        "import ast\n",
        "import bokeh.models as bmo\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from bokeh.palettes import d3\n",
        "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import entropy\n",
        "from scipy.stats import gaussian_kde\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# When calculating the distance between 2 probability densities,\n",
        "# if one probability value is 0 (or very small), the cross entropy\n",
        "# (distance) value would be infinity. This brakes the DBSCAN algorithm.\n",
        "# Replace infinity values with a large number, say 200.00  \n",
        "max_distance = 200.00\n",
        "\n",
        "# x axis values for calculating/plotting KDE of sample AF\n",
        "x_idx = np.linspace(0.00, 1.00, num=100).tolist()\n",
        "\n",
        "def get_kde_values(row):\n",
        "  return gaussian_kde(row['AF']).evaluate(x_idx).tolist()\n",
        "\n",
        "def get_kl_div(x, y):\n",
        "  return entropy(x, y)\n",
        "\n",
        "def preprocess(file_name, sep=\"\\t\"):\n",
        "\n",
        "  # Read the input file. Select only the needed columns.\n",
        "  df = pd.read_csv(file_name, sep)[['Batch', 'AF']]\n",
        "  df_in = df.copy()\n",
        "\n",
        "  # Batch stats\n",
        "  print('\\n')\n",
        "  print('Number of unique batches: {}'.format(df_in['Batch'].nunique()))\n",
        "  print('Batch minimum: {}'.format(df_in['Batch'].min()))\n",
        "  print('Batch maximum: {}'.format(df_in['Batch'].max()))\n",
        "\n",
        "  # af stats\n",
        "  print('\\n')\n",
        "  print('Number of unique af {}'.format(df_in['AF'].nunique()))\n",
        "  print('af minimum: {}'.format(df_in['AF'].min()))\n",
        "  print('af maximum: {}'.format(df_in['AF'].max()))\n",
        "\n",
        "  # Clean up data by removing rows where af is greater than 1.0\n",
        "  print('\\n')\n",
        "  print('Removing rows with AF greater than 1.0')\n",
        "  df_af = df_in[ df_in.AF <= 1.00 ]\n",
        "\n",
        "  # Pivot the data frame\n",
        "  df_piv = pd.pivot_table(df_af, index='Batch', values='AF', aggfunc=list)\n",
        "  print('df_piv.head(5)')\n",
        "  print(df_piv.head(5))\n",
        "\n",
        "  # Clean up data by removing rows where af list has only one or two element\n",
        "  # KDE calculation errors out for those\n",
        "  df_piv_clean = df_piv[ df_piv.AF.str.len() > 2]\n",
        "\n",
        "  # Calculate \n",
        "  df_piv_clean['KDE_vals'] = df_piv_clean.apply(get_kde_values, axis=1)\n",
        "\n",
        "  print('df_piv_clean.head(5)')\n",
        "  print(df_piv_clean.head(5))\n",
        "\n",
        "  return df_piv_clean\n",
        "\n",
        "# eps: \n",
        "#   The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is the most \n",
        "#   important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
        "# min_samples: \n",
        "#   The number of samples n a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
        "# metric: \n",
        "#   The metric to use when calculating distance between instances in a feature array. \n",
        "# metric_params: \n",
        "#  Additional keyword arguments for the metric function.\n",
        "\n",
        "def dbscan_clustering(file_name, sep='\\t', eps=0.5, min_samples=5, metric='euclidean', metric_params=None, distances_file_name=None, n_jobs=1):\n",
        "  df_piv_clean = preprocess(file_name, sep)\n",
        "\n",
        "  if metric == 'precomputed':\n",
        "    distances = pd.read_csv(distances_file_name, sep=sep, index_col=0)\n",
        "\n",
        "    # Replace infinity values in distances matric with a large value\n",
        "    distances.replace([np.inf], max_distance, inplace=True)\n",
        "\n",
        "    # Run DBSCAN clustering algorithm on precomputed distance matric\n",
        "    db=DBSCAN(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, n_jobs=n_jobs).fit(distances) \n",
        "  else:\n",
        "    # Run DBSCAN clustering algorithm\n",
        "    db=DBSCAN(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, n_jobs=n_jobs).fit(df_piv_clean.KDE_vals.tolist())\n",
        "\n",
        "  labels = db.labels_\n",
        "\n",
        "  # Number of clusters in labels, ignoring noise if present.\n",
        "  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "  n_noise_ = list(labels).count(-1)\n",
        "\n",
        "  print('\\n')\n",
        "  print('Number of clusters: {}'.format(n_clusters_))\n",
        "  print('Cluster labels: {}'.format(set(labels)))\n",
        "  print('Number of noise samples: {}'.format(n_noise_))\n",
        "\n",
        "  # Add Labels (and its string version) to the dataframe\n",
        "  df_piv_clean['Labels'] = labels\n",
        "\n",
        "  print('df_piv_clean.head(5)')\n",
        "  print(df_piv_clean.head(5))\n",
        "\n",
        "  return df_piv_clean\n",
        "\n",
        "def get_distance_matrix(df_in):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  row_count = df.shape[0]\n",
        "  distances = np.zeros((row_count, row_count))\n",
        "\n",
        "  for idx1 in range(row_count-1):\n",
        "    for idx2 in range(idx1+1, row_count):\n",
        "      distances[idx1][idx2] = entropy(df.iloc[idx1]['KDE_vals'], df.iloc[idx2]['KDE_vals'])\n",
        "      distances[idx2][idx1] = distances[idx1][idx2]\n",
        "  \n",
        "  df_out = pd.DataFrame(distances)\n",
        "  df_out.fillna(0.00, inplace=True)\n",
        "  distances_sum = df_out.apply(np.sum)\n",
        "  argmin = distances_sum.argmin()\n",
        "  return df_out, df.iloc[argmin]\n",
        "\n",
        "def plot_clusters(df_in, folder):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        "\n",
        "  df = df_in.copy()\n",
        "\n",
        "  num_labels = df['Labels'].nunique()\n",
        "  print('num_labels: {}'.format(num_labels))\n",
        "\n",
        "  labels = df['Labels'].unique()\n",
        "  print('labels: {}'.format(labels))\n",
        "\n",
        "  fig, axs = plt.subplots(num_labels, 2, gridspec_kw={'hspace': 1.0, 'wspace': 0.5}, figsize=(15, 15))\n",
        "\n",
        "  # Use num_labels - 1 in range, as we handle noise (-1) separately\n",
        "  for label in labels:\n",
        "    print('Label processed: {}'.format(label))\n",
        "\n",
        "    # idx used in plot axes\n",
        "    idx = 0\n",
        "    if label != -1:\n",
        "      idx = label\n",
        "    else:\n",
        "      idx = num_labels - 1\n",
        "\n",
        "    df_lbl = df[ df.Labels == label ]\n",
        "    \n",
        "    distances, cluster_center = get_distance_matrix(df_lbl)\n",
        "    print('Cluster center for label ' + str(label))\n",
        "    print(cluster_center)\n",
        "    \n",
        "    # Histogram\n",
        "    xh = cluster_center[0]\n",
        "    axs[idx][0].hist(xh, density=True)\n",
        "    axs[idx][0].title.set_text('Cluster ' + str(label) + ' (size ' + str(df_lbl.shape[0]) + ') AF histogram')\n",
        "\n",
        "    # KDE \n",
        "    xk = x_idx\n",
        "    yk = cluster_center[1]\n",
        "    axs[idx][1].plot(xk, yk)\n",
        "    axs[idx][1].title.set_text('Cluster ' + str(label) + ' (size ' + str(df_lbl.shape[0]) + ') AF density estimate')\n",
        "    \n",
        "  plt.savefig(folder + '/dbscan_' + str(num_labels) + '.png')\n",
        "  # plt.show()\n",
        "\n",
        "def get_cluster_batches(df_in, sep, folder):\n",
        "  if df_in is None or df_in.shape[0] == 0:\n",
        "    return df_in\n",
        " \n",
        "  df = df_in.copy()\n",
        "\n",
        "  labels = df['Labels'].unique()\n",
        "  print('labels: {}'.format(labels))\n",
        "\n",
        "  for label in labels:\n",
        "    df_lbl = df[ df.Labels == label ]\n",
        "    df_lbl.to_csv(folder + '/cluster_' + str(label) + '.tsv', sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91auKrzlxfl"
      },
      "source": [
        "# Run DBSCAN clustering algorithm on batch dataset\n",
        "#\n",
        "# Using 0.0075 for esp (epsilon) to yield ? clusters\n",
        "# Using default value of 5 for min_samples \n",
        "# Using get_kl_div method for metric. get_kl_div() calculates Kullback-Leibler divergence, \n",
        "#     which measures the distance between 2 proabaility distributions\n",
        "# Using None for metric_params, as the metric has no parameters\n",
        "#\n",
        "\n",
        "# 1. Cleaned the data (Removed rows with AF > 1.0)\n",
        "# 2. Pivoted the data so all AFs of a batch are listed on one line\n",
        "# 3. Calculated Kernel Density Estimates (KDE) of AFs of each batch\n",
        "#      Evaluated them on 100 data points in range of 0.0 to 1.0\n",
        "# 4. Ran DBSCAN clustering algorithm\n",
        "#      epsilon: 0.0075\n",
        "#      Used Kullback-Liebler (KL) div. to calculate distance between density estimate\n",
        "#      metric: 'precomputed'. See note below\n",
        "# 5. DBSCAN produced ? clusters\n",
        "#      Data points not assigned to any cluster marked as Noise (or cluster -1)\n",
        "# 6. For each cluster, found a representative batch\n",
        "#      Calculated KL div. between every pair of batches in a cluster\n",
        "#      Selected batch with the smallest sum of distances\n",
        "\n",
        "\n",
        "# Calculated the distance matrix. We run the code below just once, and save the \n",
        "# distance matrix to file. We pass the distance matrix file to DBSCAN. that way \n",
        "# if we modify DBSCAN parameters (say, eps or num_samples), we avoid calculating \n",
        "# the distance matrix repeatedly. Must set metric to 'precomputed'\n",
        "\n",
        "#df = preprocess('./uk_data/batch.tsv', sep='\\t')\n",
        "#distances, _ = get_distance_matrix(df)\n",
        "#distances.to_csv('./uk_data/distances_batch.tsv', sep='\\t')\n",
        "\n",
        "def dbscan_clustering_wrapper(eps=0.0085, \n",
        "                              min_samples=7,\n",
        "                              path = '/content/gdrive/MyDrive/Colab Notebooks/Clustering/',\n",
        "                              data_file='batch.tsv',\n",
        "                              sep='\\t',\n",
        "                              data_folder = 'batch_data',\n",
        "                              results_folder = 'batch_results',\n",
        "                              metric='precomputed'):\n",
        "  folder = str(min_samples) + '_' + str(eps)\n",
        "  print('folder: {}'.format(folder))\n",
        "  full_path = os.path.join(path, results_folder, folder)\n",
        "  print('full_path: {}'.format(full_path))\n",
        "  os.mkdir(full_path)\n",
        "\n",
        "  full_data_folder = os.path.join(path, data_folder)\n",
        "  full_results_folder = os.path.join(path, results_folder, folder)\n",
        "\n",
        "  df = dbscan_clustering(file_name=full_data_folder+'/'+data_file,\n",
        "                         sep=sep, \n",
        "                         eps=eps, \n",
        "                         min_samples=min_samples, \n",
        "                         #metric=get_kl_div,\n",
        "                         metric=metric,\n",
        "                         metric_params=None,\n",
        "                         distances_file_name=full_data_folder+'/distances_'+data_file)\n",
        "\n",
        "  df.to_csv(full_results_folder + '/all_clusters_eps_' + str(eps) + '_min_samples_' + str(min_samples) + '.tsv', sep=sep)\n",
        "  plot_clusters(df, folder=full_results_folder)\n",
        "  get_cluster_batches(df_in=df, sep=sep, folder=full_results_folder)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run DBSCAN algorithm on 'data_file' in 'data_folder' and save the results to 'results_folder'. \n",
        "dbscan_clustering_wrapper(eps=0.0250, \n",
        "                          min_samples=3,\n",
        "                          path = '/content/gdrive/MyDrive/Colab Notebooks/Clustering/',\n",
        "                          data_file='batch.tsv',\n",
        "                          sep='\\t',\n",
        "                          data_folder = 'batch_data',\n",
        "                          results_folder = 'batch_results',\n",
        "                          metric='precomputed')"
      ],
      "metadata": {
        "id": "5Pc3aNYx5Ju9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram for number of samples in a batch\n",
        "\n",
        "# Read the input file. Select only the needed columns.\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Clustering/batch_data/batch.tsv', sep='\\t')[['Batch','Sample']]\n",
        "df_no_dup = df.drop_duplicates()\n",
        "print(df_no_dup.head(20))\n",
        "count = df_no_dup.groupby('Batch').count()\n",
        "print(type(count))\n",
        "print(count)\n",
        "print(count['Sample'].values)\n",
        "plt.hist(count['Sample'].values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "id": "lh9rpR6I56yw",
        "outputId": "de51a50f-682f-4dc9-c659-c835e6212631"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Batch      Sample\n",
            "0    0ce66d803a055fef  ERR5549561\n",
            "42   0ce66d803a055fef  ERR5549562\n",
            "83   0ce66d803a055fef  ERR5549563\n",
            "120  0ce66d803a055fef  ERR5549564\n",
            "163  0ce66d803a055fef  ERR5549565\n",
            "210  0ce66d803a055fef  ERR5549566\n",
            "248  0ce66d803a055fef  ERR5549567\n",
            "285  0ce66d803a055fef  ERR5549568\n",
            "323  0ce66d803a055fef  ERR5549569\n",
            "367  0ce66d803a055fef  ERR5643745\n",
            "406  87f9807bf2e0660b  ERR5549545\n",
            "444  87f9807bf2e0660b  ERR5549546\n",
            "474  87f9807bf2e0660b  ERR5549547\n",
            "510  87f9807bf2e0660b  ERR5549548\n",
            "546  87f9807bf2e0660b  ERR5549549\n",
            "586  87f9807bf2e0660b  ERR5549550\n",
            "596  87f9807bf2e0660b  ERR5549551\n",
            "637  87f9807bf2e0660b  ERR5549552\n",
            "677  87f9807bf2e0660b  ERR5549553\n",
            "714  87f9807bf2e0660b  ERR5549554\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "                  Sample\n",
            "Batch                   \n",
            "00ac59cfc7cd16df      18\n",
            "010b73d60883c859     354\n",
            "010db33f449760f6      94\n",
            "01260d0ae85deee5      90\n",
            "01352766709fd0e9      15\n",
            "...                  ...\n",
            "ffb48b3acdcc5583     351\n",
            "ffc5d29b85a9e89b       1\n",
            "ffca98c84dc38f2f     360\n",
            "ffefa23d64a12f74     327\n",
            "fff87bb5eb22b278     338\n",
            "\n",
            "[1451 rows x 1 columns]\n",
            "[ 18 354  94 ... 360 327 338]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([385., 126.,  73.,  35.,  30.,  60., 124., 438., 178.,   2.]),\n",
              " array([  1. ,  44.9,  88.8, 132.7, 176.6, 220.5, 264.4, 308.3, 352.2,\n",
              "        396.1, 440. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}